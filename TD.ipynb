{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib scipy scikit-learn pytest -q\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data handling and processing\n",
    "from datetime import datetime  # Handling date and time-related operations\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "\n",
    "# Numerical Computation and Linear Algebra\n",
    "from scipy.linalg import toeplitz, solve, inv, pinv  # Linear algebra tools for matrix operations\n",
    "from scipy.optimize import minimize, minimize_scalar  # Optimization functions for parameter estimation\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>hf_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  hf_y\n",
       "0 2024-01-01  10.0\n",
       "1 2024-01-02   NaN\n",
       "2 2024-01-03  15.0\n",
       "3 2024-01-04   NaN\n",
       "4 2024-01-05  20.0\n",
       "5 2024-01-06  22.0\n",
       "6 2024-01-07   NaN\n",
       "7 2024-01-08  30.0\n",
       "8 2024-01-09   NaN\n",
       "9 2024-01-10  40.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TimeSeriesPreprocessor:\n",
    "    \"\"\"\n",
    "    Clase para procesar series de tiempo a partir de un conjunto de valores num√©ricos,\n",
    "    generando un rango de fechas y aplicando interpolaci√≥n para valores faltantes.\n",
    "\n",
    "    Atributos:\n",
    "        num_array (list o np.ndarray): Lista o array de valores num√©ricos.\n",
    "        start_date (str o datetime): Fecha de inicio de la serie de tiempo.\n",
    "        end_date (str o datetime): Fecha de fin de la serie de tiempo.\n",
    "        freq (str): Frecuencia de la serie de tiempo (ej. 'D' para diario, 'H' para horas, etc.).\n",
    "        interp_method (str): M√©todo de interpolaci√≥n para manejar valores faltantes (por defecto 'linear').\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_array, start_date, end_date, freq, interp_method='linear'):\n",
    "        \"\"\"\n",
    "        Inicializa la clase con los par√°metros requeridos.\n",
    "\n",
    "        :param num_array: Lista o array de valores num√©ricos.\n",
    "        :param start_date: Fecha de inicio de la serie de tiempo (str o datetime).\n",
    "        :param end_date: Fecha de fin de la serie de tiempo (str o datetime).\n",
    "        :param freq: Frecuencia de la serie de tiempo (ej. 'D' para diario, 'H' para horas, etc.).\n",
    "        :param interp_method: M√©todo de interpolaci√≥n para manejar valores faltantes (por defecto 'linear').\n",
    "        \"\"\"\n",
    "        self.num_array = np.array(num_array, dtype=np.float64)  # Convierte a array NumPy para manejo eficiente\n",
    "        self.start_date = pd.to_datetime(start_date)  # Convierte la fecha de inicio a formato datetime\n",
    "        self.end_date = pd.to_datetime(end_date)  # Convierte la fecha de fin a formato datetime\n",
    "        self.freq = freq  # Define la frecuencia de la serie de tiempo\n",
    "        self.interp_method = interp_method  # Define el m√©todo de interpolaci√≥n para valores faltantes\n",
    "\n",
    "        # Validaciones b√°sicas\n",
    "        if len(self.num_array) == 0:\n",
    "            raise ValueError(\"El array de valores num√©ricos no puede estar vac√≠o.\")\n",
    "\n",
    "        if self.start_date >= self.end_date:\n",
    "            raise ValueError(\"La fecha de inicio debe ser anterior a la fecha de fin.\")\n",
    "\n",
    "        if not isinstance(self.freq, str):\n",
    "            raise TypeError(\"La frecuencia debe ser una cadena de caracteres (ej. 'D', 'H').\")\n",
    "\n",
    "\n",
    "    def _convert_to_pd_series(self):\n",
    "        \"\"\"\n",
    "        Convierte la lista de valores num√©ricos en una serie de Pandas.\n",
    "\n",
    "        :return: Serie de Pandas con valores num√©ricos o None en caso de error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.Series(self.num_array, dtype=float)\n",
    "        except Exception as e:\n",
    "            print(f\"üö® Error al convertir los valores num√©ricos en una serie de Pandas: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _create_pd_datetime(self):\n",
    "        \"\"\"\n",
    "        Genera un rango de fechas con la frecuencia especificada.\n",
    "\n",
    "        :return: Serie de Pandas con fechas o None en caso de error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.date_range(start=self.start_date, end=self.end_date, freq=self.freq)\n",
    "        except Exception as e:\n",
    "            print(f\"üìÖ Error al generar la serie de fechas: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_time_series(self, col_name):\n",
    "        \"\"\"\n",
    "        Crea una serie de tiempo combinando valores num√©ricos con su correspondiente rango de fechas.\n",
    "\n",
    "        :return: DataFrame de Pandas con dos columnas: 'Date' y 'Value', o None en caso de error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dates = self._create_pd_datetime()\n",
    "            values = self._convert_to_pd_series()\n",
    "\n",
    "            if dates is None or values is None:\n",
    "                print(\"‚ùå Error: No se pudo generar la serie de tiempo debido a errores previos.\")\n",
    "                return None\n",
    "\n",
    "            if len(dates) != len(values):\n",
    "                print(\"‚ö†Ô∏è Error: La cantidad de valores num√©ricos no coincide con la cantidad de fechas generadas.\")\n",
    "                return None\n",
    "\n",
    "            return pd.DataFrame({'Date': dates, col_name: values})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error en la creaci√≥n de la serie de tiempo: {e}\")\n",
    "            return None\n",
    "\n",
    "    def interpolate_and_fill(self, series):\n",
    "        \"\"\"\n",
    "        Aplica interpolaci√≥n y relleno de valores faltantes en una serie de Pandas.\n",
    "\n",
    "        :param series: Serie de Pandas con posibles valores NaN.\n",
    "        :return: Serie de Pandas con valores interpolados o None en caso de error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            series = series.copy()  # Evita modificar el original\n",
    "            series = series.interpolate(method=self.interp_method)  # Interpolaci√≥n\n",
    "            series = series.ffill().bfill()  # Rellena hacia adelante y hacia atr√°s\n",
    "            return series\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error en la interpolaci√≥n y relleno de valores: {e}\")\n",
    "        \n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "# üîπ Definimos par√°metros para la serie de tiempo\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = \"2024-01-10\"\n",
    "freq = \"D\"  # Diaria\n",
    "\n",
    "# üîπ Creamos un array de valores num√©ricos con algunos NaN simulando datos faltantes\n",
    "num_values = np.array([10, np.nan, 15, np.nan, 20, 22, np.nan, 30, np.nan, 40])\n",
    "\n",
    "# üîπ Instanciamos la clase\n",
    "preprocessor = TimeSeriesPreprocessor(num_values, start_date, end_date, freq)\n",
    "\n",
    "# üîπ Creamos la serie de tiempo\n",
    "time_series_df = preprocessor.create_time_series(col_name = \"hf_y\")\n",
    "\n",
    "time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesUnifier:\n",
    "    \"\"\"\n",
    "    Clase para procesar y unir series de tiempo de alta y baja frecuencia.\n",
    "    Se preparan adecuadamente para su posterior uso en desagregaci√≥n.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ts_high_freq, start_date_hf, end_date_hf, freq_hf,\n",
    "                 ts_low_freq, start_date_lf, end_date_lf, freq_lf):\n",
    "        \"\"\"\n",
    "        Inicializa la clase con series de alta y baja frecuencia.\n",
    "\n",
    "        :param ts_high_freq: Serie de tiempo de alta frecuencia (pandas Series o DataFrame).\n",
    "        :param start_date_hf: Fecha de inicio de la serie de alta frecuencia (str o datetime).\n",
    "        :param end_date_hf: Fecha de fin de la serie de alta frecuencia (str o datetime).\n",
    "        :param freq_hf: Frecuencia de la serie de alta frecuencia (str, ej. 'D', 'H').\n",
    "        :param ts_low_freq: Serie de tiempo de baja frecuencia (pandas Series o DataFrame).\n",
    "        :param start_date_lf: Fecha de inicio de la serie de baja frecuencia (str o datetime).\n",
    "        :param end_date_lf: Fecha de fin de la serie de baja frecuencia (str o datetime).\n",
    "        :param freq_lf: Frecuencia de la serie de baja frecuencia (str, ej. 'M', 'Q').\n",
    "        \"\"\"\n",
    "\n",
    "        # Convertir fechas a datetime\n",
    "        self.start_date_hf = pd.to_datetime(start_date_hf)\n",
    "        self.end_date_hf = pd.to_datetime(end_date_hf)\n",
    "        self.start_date_lf = pd.to_datetime(start_date_lf)\n",
    "        self.end_date_lf = pd.to_datetime(end_date_lf)\n",
    "\n",
    "        # Validar rangos de fechas\n",
    "        if self.start_date_hf > self.end_date_hf:\n",
    "            raise ValueError(\"La fecha de inicio de alta frecuencia no puede ser mayor que la de fin.\")\n",
    "        if self.start_date_lf > self.end_date_lf:\n",
    "            raise ValueError(\"La fecha de inicio de baja frecuencia no puede ser mayor que la de fin.\")\n",
    "\n",
    "        # Validar tipos de series de tiempo\n",
    "        if not isinstance(ts_high_freq, (pd.Series, pd.DataFrame)):\n",
    "            raise TypeError(\"La serie de alta frecuencia debe ser un pandas Series o DataFrame.\")\n",
    "        if not isinstance(ts_low_freq, (pd.Series, pd.DataFrame)):\n",
    "            raise TypeError(\"La serie de baja frecuencia debe ser un pandas Series o DataFrame.\")\n",
    "\n",
    "        # Guardar series de tiempo\n",
    "        self.ts_high_freq = ts_high_freq\n",
    "        self.ts_low_freq = ts_low_freq\n",
    "        self.freq_hf = freq_hf\n",
    "        self.freq_lf = freq_lf\n",
    "\n",
    "    def _process_time_series(self):\n",
    "        \"\"\"\n",
    "        Procesa y unifica las series de alta y baja frecuencia.\n",
    "        \"\"\"\n",
    "\n",
    "        # Crear la serie de alta frecuencia\n",
    "        preprocessor_hf = TimeSeriesPreprocessor(self.ts_high_freq, self.start_date_hf, self.end_date_hf, self.freq_hf)\n",
    "        time_series_hf_df = preprocessor_hf.create_time_series(col_name = \"High_freq_y\")\n",
    "\n",
    "        # Crear la serie de baja frecuencia\n",
    "        preprocessor_lf = TimeSeriesPreprocessor(self.ts_low_freq, self.start_date_lf, self.end_date_lf, self.freq_lf)\n",
    "        time_series_lf_df = preprocessor_lf.create_time_series(col_name = \"Low_freq_y\")\n",
    "\n",
    "        # Validar que ambas series fueron creadas correctamente\n",
    "        if time_series_hf_df is None or time_series_lf_df is None:\n",
    "            raise ValueError(\"No se pudieron procesar correctamente una o ambas series de tiempo.\")\n",
    "\n",
    "        # Unir las series por la columna de fecha\n",
    "        time_series_merged = pd.merge(left=time_series_hf_df, right=time_series_lf_df, on=\"Date\", how=\"left\")\n",
    "\n",
    "        return time_series_merged\n",
    "\n",
    "    def _infer_start_value(self, date_index: pd.DatetimeIndex, periods_per_year: int) -> int:\n",
    "        \"\"\"\n",
    "        Infers the start_value based on the position of the first date within the available data range.\n",
    "\n",
    "        Args:\n",
    "            date_index (pd.DatetimeIndex): List of generated dates.\n",
    "            periods_per_year (int): Expected number of periods per year.\n",
    "\n",
    "        Returns:\n",
    "            int: Inferred start value.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if len(date_index) == 0:\n",
    "                raise ValueError(\"El √≠ndice de fechas est√° vac√≠o.\")\n",
    "\n",
    "            first_date = date_index[0]  # Primera fecha de la serie\n",
    "            last_date = date_index[-1]  # √öltima fecha de la serie\n",
    "\n",
    "            # Determinar en qu√© parte del ciclo anual cae\n",
    "            if periods_per_year == 365:  # Diario\n",
    "                return first_date.timetuple().tm_yday  # D√≠a del a√±o (1-365)\n",
    "            elif periods_per_year == 52:  # Semanal\n",
    "                return first_date.isocalendar()[1]  # Semana del a√±o (1-52)\n",
    "            elif periods_per_year == 12:  # Mensual\n",
    "                return first_date.month  # Mes del a√±o (1-12)\n",
    "            elif periods_per_year == 4:  # Trimestral\n",
    "                num_trimestres_disponibles = ((last_date.month - first_date.month) // 3) + 1\n",
    "                return min((first_date.month - 1) // 3 + 1, num_trimestres_disponibles)  # Evita ciclos incorrectos\n",
    "            elif periods_per_year == 1:  # Anual\n",
    "                return 1  # Siempre comienza en 1\n",
    "            else:\n",
    "                raise ValueError(f\"Frecuencia no reconocida: {periods_per_year}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error en _infer_start_value: {e}\")\n",
    "            return 1  # Valor por defecto para evitar fallos\n",
    "        \n",
    "\n",
    "    def _generate_indicator(self, num_obs: int, periods_per_year: int, start_value: int) -> np.array:\n",
    "        \"\"\"\n",
    "        Generates a cyclic indicator based on the number of periods per year.\n",
    "\n",
    "        Args:\n",
    "            num_obs (int): Total number of observations.\n",
    "            periods_per_year (int): Number of periods in a year.\n",
    "            start_value (int): Initial value of the indicator.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Vector with the cyclic indicator.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if num_obs <= 0:\n",
    "                raise ValueError(\"El n√∫mero de observaciones debe ser mayor que cero.\")\n",
    "            if periods_per_year <= 0:\n",
    "                raise ValueError(\"El n√∫mero de periodos por a√±o debe ser mayor que cero.\")\n",
    "\n",
    "            # Ajustar start_value para estar dentro del rango disponible\n",
    "            start_value = (start_value - 1) % periods_per_year + 1\n",
    "\n",
    "            # Generar la secuencia c√≠clica dentro del rango observado\n",
    "            indicator = (np.arange(num_obs) + start_value - 1) % min(num_obs, periods_per_year) + 1\n",
    "            return indicator\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error en _generate_indicator: {e}\")\n",
    "            return np.array([])\n",
    "        \n",
    "    def _infer_frequency(self, freq: str) -> int:\n",
    "        \"\"\"\n",
    "        Infers the annual frequency based on the given frequency string.\n",
    "\n",
    "        Args:\n",
    "            freq (str): Frequency of the time series (e.g., 'D', 'W', 'M', 'Q', 'A').\n",
    "\n",
    "        Returns:\n",
    "            int: Estimated annual frequency (365 for daily, 52 for weekly, 12 for monthly, etc.).\n",
    "        \"\"\"\n",
    "        # Convertimos la frecuencia a may√∫sculas para evitar errores por diferencia de capitalizaci√≥n\n",
    "        freq = freq.upper()\n",
    "\n",
    "        # Verificamos si contiene la letra clave para cada periodo\n",
    "        if \"D\" in freq:\n",
    "            return 365  # Diario\n",
    "        elif \"W\" in freq:\n",
    "            return 52  # Semanal\n",
    "        elif \"M\" in freq:\n",
    "            return 12  # Mensual\n",
    "        elif \"Q\" in freq:\n",
    "            return 4   # Trimestral\n",
    "        elif \"A\" in freq or \"Y\" in freq:  # \"A\" para anual, \"Y\" si usa \"Yearly\"\n",
    "            return 1   # Anual\n",
    "        else:\n",
    "            return 1   # Valor por defecto: Anual si no se reconoce\n",
    "\n",
    "        \n",
    "    def generate_ts_combined(self):\n",
    "    \n",
    "        time_series_merged = self._process_time_series()\n",
    "        time_series_merged = time_series_merged.set_index(\"Date\")\n",
    "\n",
    "        freq = self._infer_frequency(self.freq_hf)\n",
    "        start_value = self._infer_start_value(time_series_merged.index, freq)\n",
    "\n",
    "        # Generar el indicador c√≠clico\n",
    "        time_series_merged[\"Indicator\"] = self._generate_indicator(len(time_series_merged), freq, start_value)\n",
    "\n",
    "        # Interpolaci√≥n de valores faltantes (sin inplace)\n",
    "        time_series_merged[\"Low_freq_y\"] = time_series_merged[\"Low_freq_y\"].interpolate(method=\"linear\")\n",
    "\n",
    "        # Rellenar valores NaN restantes (sin inplace)\n",
    "        time_series_merged[\"Low_freq_y\"] = time_series_merged[\"Low_freq_y\"].ffill()\n",
    "        time_series_merged[\"Low_freq_y\"] = time_series_merged[\"Low_freq_y\"].bfill()\n",
    "\n",
    "        return time_series_merged\n",
    "\n",
    "\n",
    "# üìÜ Corregir la cantidad de valores para que coincidan con las fechas\n",
    "\n",
    "start_date_hf = \"2001-01-01\"\n",
    "end_date_hf = \"2024-01-01\"\n",
    "\n",
    "start_date_lf = \"2001-01-01\"\n",
    "end_date_lf = \"2024-01-01\"\n",
    "\n",
    "freq_hf = \"ME\"\n",
    "freq_lf = \"YE\"\n",
    "\n",
    "# Generar correctamente los valores de alta frecuencia (mensual)\n",
    "num_months = pd.date_range(start_date_hf, end_date_hf, freq=freq_hf).shape[0]\n",
    "num_values_hf = np.linspace(100, 350, num_months)  # Generar valores progresivos\n",
    "\n",
    "# Generar correctamente los valores de baja frecuencia (anual)\n",
    "num_years = pd.date_range(start_date_lf, end_date_lf, freq=freq_lf).shape[0]\n",
    "num_values_lf = np.linspace(150, 500, num_years)  # Valores anuales\n",
    "\n",
    "# üìä Convertir a pandas Series\n",
    "ts_high_freq = pd.Series(num_values_hf)\n",
    "ts_low_freq = pd.Series(num_values_lf)\n",
    "\n",
    "# üîπ Crear instancia de TimeSeriesUnifier con las series corregidas\n",
    "unifier = TimeSeriesUnifier(ts_high_freq, start_date_hf, end_date_hf, freq_hf,\n",
    "                            ts_low_freq, start_date_lf, end_date_lf, freq_lf)\n",
    "\n",
    "# Procesar la serie unificada\n",
    "time_series_merged = unifier.generate_ts_combined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High_freq_y</th>\n",
       "      <th>Low_freq_y</th>\n",
       "      <th>Indicator</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001-01-31</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-02-28</th>\n",
       "      <td>100.909091</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-03-31</th>\n",
       "      <td>101.818182</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-04-30</th>\n",
       "      <td>102.727273</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-05-31</th>\n",
       "      <td>103.636364</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-31</th>\n",
       "      <td>346.363636</td>\n",
       "      <td>494.696970</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-30</th>\n",
       "      <td>347.272727</td>\n",
       "      <td>496.022727</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-31</th>\n",
       "      <td>348.181818</td>\n",
       "      <td>497.348485</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-30</th>\n",
       "      <td>349.090909</td>\n",
       "      <td>498.674242</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            High_freq_y  Low_freq_y  Indicator\n",
       "Date                                          \n",
       "2001-01-31   100.000000  150.000000          1\n",
       "2001-02-28   100.909091  150.000000          2\n",
       "2001-03-31   101.818182  150.000000          3\n",
       "2001-04-30   102.727273  150.000000          4\n",
       "2001-05-31   103.636364  150.000000          5\n",
       "...                 ...         ...        ...\n",
       "2023-08-31   346.363636  494.696970          8\n",
       "2023-09-30   347.272727  496.022727          9\n",
       "2023-10-31   348.181818  497.348485         10\n",
       "2023-11-30   349.090909  498.674242         11\n",
       "2023-12-31   350.000000  500.000000         12\n",
       "\n",
       "[276 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TimeSeriesProcessor:\n",
    "    \"\"\"\n",
    "    Class to process, interpolate, and merge time series of different frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ts_high_freq, start_date_hf, end_date_hf, freq_hf,\n",
    "                 ts_low_freq, start_date_lf, end_date_lf, freq_lf, interp_method='nearest'):\n",
    "        \"\"\"\n",
    "        Initializes the class with high- and low-frequency time series.\n",
    "\n",
    "        :param ts_high_freq: High-frequency time series (array or list).\n",
    "        :param start_date_hf: Start date of the high-frequency series (str or datetime).\n",
    "        :param end_date_hf: End date of the high-frequency series (str or datetime).\n",
    "        :param freq_hf: Frequency of the high-frequency series (e.g., 'D' for daily, 'H' for hourly).\n",
    "        :param ts_low_freq: Low-frequency time series (array or list).\n",
    "        :param start_date_lf: Start date of the low-frequency series (str or datetime).\n",
    "        :param end_date_lf: End date of the low-frequency series (str or datetime).\n",
    "        :param freq_lf: Frequency of the low-frequency series (e.g., 'M' for monthly, 'Q' for quarterly).\n",
    "        :param interp_method: Interpolation method for missing values (default is 'nearest').\n",
    "        \"\"\"\n",
    "        self.freq_hf = freq_hf  # Stores the frequency of the high-frequency series\n",
    "        self.freq_lf = freq_lf  # Stores the frequency of the low-frequency series\n",
    "        self.interp_method = interp_method  # Defines the interpolation method\n",
    "\n",
    "        # Create DataFrames for time series using a helper method\n",
    "        self.ts_high_freq = self._create_time_series(ts_high_freq, start_date_hf, end_date_hf, freq_hf, \"High_freq\")\n",
    "        self.ts_low_freq = self._create_time_series(ts_low_freq, start_date_lf, end_date_lf, freq_lf, \"Low_freq\")\n",
    "\n",
    "        # Ensure both time series were successfully created\n",
    "        if self.ts_high_freq is None or self.ts_low_freq is None:\n",
    "            raise ValueError(\"Error in creating time series.\")\n",
    "\n",
    "    def _create_time_series(self, values, start_date, end_date, freq, col_name):\n",
    "        \"\"\"\n",
    "        Creates a pandas DataFrame with a time series.\n",
    "\n",
    "        :param values: List of numerical values.\n",
    "        :param start_date: Start date of the time series.\n",
    "        :param end_date: End date of the time series.\n",
    "        :param freq: Frequency of the time series.\n",
    "        :param col_name: Name of the values column.\n",
    "        :return: Pandas DataFrame with 'Date' and the time series.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate date range based on start, end, and frequency\n",
    "            dates = pd.date_range(start=start_date, end=end_date, freq=freq)\n",
    "\n",
    "            # Ensure the number of generated dates matches the number of values\n",
    "            if len(dates) != len(values):\n",
    "                raise ValueError(f\"Error: Number of values does not match generated dates for {col_name}.\")\n",
    "\n",
    "            # Create DataFrame with dates and values\n",
    "            df = pd.DataFrame({'Date': dates, col_name: values})\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating time series {col_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _interpolate_and_fill(self, series):\n",
    "        \"\"\"\n",
    "        Applies interpolation and fills missing values.\n",
    "\n",
    "        :param series: Pandas Series with possible NaN values.\n",
    "        :return: Interpolated Series without NaN values.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Apply interpolation using the defined method and fill missing values forward and backward\n",
    "            series = series.interpolate(method=self.interp_method).ffill().bfill()\n",
    "            return series\n",
    "        except Exception as e:\n",
    "            print(f\"Error in interpolation and filling: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _infer_frequency(self, freq):\n",
    "        \"\"\"\n",
    "        Infers the number of periods per year based on the series frequency.\n",
    "\n",
    "        :param freq: Frequency of the time series (e.g., 'D', 'W', 'M', 'Q', 'A').\n",
    "        :return: Number of periods per year.\n",
    "        \"\"\"\n",
    "        # Mapping different time frequencies to their corresponding periods per year\n",
    "        freq_map = {\"D\": 365, \"W\": 52, \"M\": 12, \"Q\": 4, \"A\": 1}\n",
    "        return freq_map.get(freq[0].upper(), 1)  # Default to 1 if not found\n",
    "\n",
    "    def _infer_start_value(self, date_index, periods_per_year):\n",
    "        \"\"\"\n",
    "        Infers the initial value of the cyclic indicator based on the date.\n",
    "\n",
    "        :param date_index: Date index of the time series.\n",
    "        :param periods_per_year: Number of periods per year.\n",
    "        :return: Initial value of the cyclic indicator.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            first_date = date_index[0]  # Retrieve the first date of the series\n",
    "\n",
    "            # Assign the starting value based on the frequency\n",
    "            if periods_per_year == 365:\n",
    "                return first_date.timetuple().tm_yday  # Day of the year\n",
    "            elif periods_per_year == 52:\n",
    "                return first_date.isocalendar()[1]  # ISO week number\n",
    "            elif periods_per_year == 12:\n",
    "                return first_date.month  # Month number\n",
    "            elif periods_per_year == 4:\n",
    "                return (first_date.month - 1) // 3 + 1  # Quarter of the year\n",
    "            else:\n",
    "                return 1  # Default value if none match\n",
    "        except Exception as e:\n",
    "            print(f\"Error in _infer_start_value: {e}\")\n",
    "            return 1  # Return a default value in case of failure\n",
    "\n",
    "    def _generate_indicator(self, num_obs, periods_per_year, start_value):\n",
    "        \"\"\"\n",
    "        Generates a cyclic indicator based on the number of periods per year.\n",
    "\n",
    "        :param num_obs: Total number of observations.\n",
    "        :param periods_per_year: Expected periods per year.\n",
    "        :param start_value: Initial value of the indicator.\n",
    "        :return: Vector with the cyclic indicator.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Adjust the start value to ensure it fits within the cycle\n",
    "            start_value = (start_value - 1) % periods_per_year + 1\n",
    "\n",
    "            # Create a cyclic pattern by repeating values in the expected period\n",
    "            return (np.arange(num_obs) + start_value - 1) % periods_per_year + 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error in _generate_indicator: {e}\")\n",
    "            return np.array([])  # Return an empty array in case of failure\n",
    "\n",
    "    def process_and_merge_series(self):\n",
    "        \"\"\"\n",
    "        Processes and integrates high- and low-frequency time series.\n",
    "\n",
    "        :return: Pandas DataFrame with the combined series.\n",
    "        \"\"\"\n",
    "        # Merge high- and low-frequency series using a left join on the date\n",
    "        merged_df = pd.merge(self.ts_high_freq, self.ts_low_freq, on=\"Date\", how=\"left\")\n",
    "\n",
    "        # Interpolate and fill missing values in the low-frequency series\n",
    "        merged_df[\"Low_freq\"] = self._interpolate_and_fill(merged_df[\"Low_freq\"])\n",
    "\n",
    "        # Infer the number of periods per year based on high-frequency data\n",
    "        periods_per_year = self._infer_frequency(self.freq_hf)\n",
    "\n",
    "        # Determine the starting value of the cyclic indicator\n",
    "        start_value = self._infer_start_value(merged_df[\"Date\"], periods_per_year)\n",
    "\n",
    "        # Generate the cyclic indicator based on inferred parameters\n",
    "        merged_df[\"Indicator\"] = self._generate_indicator(len(merged_df), periods_per_year, start_value)\n",
    "\n",
    "        # Extract year from the date column and rename it as 'index'\n",
    "        merged_df[\"Year\"] = pd.to_numeric(merged_df[\"Date\"].dt.year).astype(int)\n",
    "\n",
    "        # Rename columns to match expected output format\n",
    "        merged_df = merged_df.rename(columns={\"Year\": \"Index\",\n",
    "                                              \"High_freq\": \"X\",\n",
    "                                              \"Low_freq\": \"y\",\n",
    "                                              \"Indicator\": \"Grain\"})\n",
    "\n",
    "        # Return only the required columns\n",
    "        return merged_df[['Index', 'Grain', 'X', 'y']]\n",
    "\n",
    "\n",
    "# üìÜ Definir fechas y frecuencias\n",
    "start_date_hf = \"2001-01-01\"\n",
    "end_date_hf = \"2024-01-01\"\n",
    "start_date_lf = \"2001-01-01\"\n",
    "end_date_lf = \"2024-01-01\"\n",
    "freq_hf = \"ME\"  # Mensual\n",
    "freq_lf = \"YE\"  # Anual\n",
    "\n",
    "# Generar valores de alta y baja frecuencia\n",
    "num_months = pd.date_range(start_date_hf, end_date_hf, freq=freq_hf).shape[0]\n",
    "num_years = pd.date_range(start_date_lf, end_date_lf, freq=freq_lf).shape[0]\n",
    "values_hf = np.linspace(100, 350, num_months)\n",
    "values_lf = np.linspace(150, 500, num_years)\n",
    "\n",
    "# Instanciar y procesar la serie unificada\n",
    "processor = TimeSeriesProcessor(values_hf, start_date_hf, end_date_hf, freq_hf,\n",
    "                                values_lf, start_date_lf, end_date_lf, freq_lf)\n",
    "merged_series = processor.process_and_merge_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Grain</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>100.909091</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>101.818182</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>102.727273</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>103.636364</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>346.363636</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>347.272727</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>348.181818</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>349.090909</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index  Grain           X      y\n",
       "0     2001      1  100.000000  150.0\n",
       "1     2001      2  100.909091  150.0\n",
       "2     2001      3  101.818182  150.0\n",
       "3     2001      4  102.727273  150.0\n",
       "4     2001      5  103.636364  150.0\n",
       "..     ...    ...         ...    ...\n",
       "271   2023      8  346.363636  500.0\n",
       "272   2023      9  347.272727  500.0\n",
       "273   2023     10  348.181818  500.0\n",
       "274   2023     11  349.090909  500.0\n",
       "275   2023     12  350.000000  500.0\n",
       "\n",
       "[276 rows x 4 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDisaggregation:\n",
    "    \"\"\"\n",
    "    Class for temporal disaggregation of time series data using various statistical methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conversion=\"sum\", min_rho_boundarie=-0.9, max_rho_boundarie=0.99, apply_adjustment = True):\n",
    "        \"\"\"\n",
    "        Initializes the TemporalDisaggregation class with parameters for disaggregation.\n",
    "\n",
    "        Parameters:\n",
    "            conversion (str): Specifies the type of aggregation method to ensure consistency during disaggregation.\n",
    "                             Options include:\n",
    "                                - \"sum\": Ensures that the sum of the disaggregated values matches the low-frequency series.\n",
    "                                - \"average\": Ensures the average value remains consistent.\n",
    "                                - \"first\": Preserves the first observed value in each aggregated period.\n",
    "                                - \"last\": Maintains the last observed value in each aggregated period.\n",
    "\n",
    "            min_rho_boundarie (float): The minimum allowed value for the autoregressive parameter (rho).\n",
    "                                       This is used to constrain the estimation process to avoid instability.\n",
    "\n",
    "            max_rho_boundarie (float): The maximum allowed value for the autoregressive parameter (rho).\n",
    "                                       It prevents the estimation from diverging or producing unreliable results.\n",
    "            \n",
    "            apply_adjustment (bool): The bool value that reflects whether the series must be corrected or not.\n",
    "                                       Negative values must be transformed.\n",
    "\n",
    "        Attributes:\n",
    "            self.conversion (str): Stores the specified conversion method for future computations.\n",
    "            self.min_rho_boundarie (float): Lower bound for rho to ensure a stable disaggregation process.\n",
    "            self.max_rho_boundarie (float): Upper bound for rho to prevent extreme values.\n",
    "            self.apply_adjustment (bool): Boolean for negative values adjustment\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the chosen conversion method for disaggregation\n",
    "        self.conversion = conversion  \n",
    "\n",
    "        # Set the lower boundary for the autoregressive parameter (rho)\n",
    "        self.min_rho_boundarie = min_rho_boundarie  \n",
    "\n",
    "        # Set the upper boundary for the autoregressive parameter (rho)\n",
    "        self.max_rho_boundarie = max_rho_boundarie \n",
    "\n",
    "        # Set the negative values adjustment\n",
    "        self.apply_adjustment = apply_adjustment\n",
    "\n",
    "\n",
    "    def build_conversion_matrix(self, df):\n",
    "        \"\"\"\n",
    "        Constructs a conversion matrix to map high-frequency data to low-frequency data.\n",
    "\n",
    "        This matrix ensures that the disaggregated series maintains consistency with the \n",
    "        specified aggregation method.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): A DataFrame containing time series data with \"Index\" and \"Grain\" columns.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Conversion matrix for temporal disaggregation.\n",
    "        \"\"\"\n",
    "\n",
    "        def get_conversion_vector(size, conversion):\n",
    "            \"\"\"\n",
    "            Generates a conversion vector based on the specified aggregation method.\n",
    "\n",
    "            Parameters:\n",
    "                size (int): The number of high-frequency observations corresponding to a single low-frequency period.\n",
    "                conversion (str): The method of aggregation ('sum', 'average', 'first', 'last').\n",
    "\n",
    "            Returns:\n",
    "                np.ndarray: A vector that defines how high-frequency data should be aggregated.\n",
    "            \"\"\"\n",
    "            if conversion == \"sum\":\n",
    "                return np.ones(size)  # Assigns equal weight to all values to preserve the sum.\n",
    "            elif conversion == \"average\":\n",
    "                return np.ones(size) / size  # Distributes weights equally to maintain the average.\n",
    "            elif conversion == \"first\":\n",
    "                vec = np.zeros(size)\n",
    "                vec[0] = 1  # Assigns weight only to the first observation.\n",
    "                return vec\n",
    "            elif conversion == \"last\":\n",
    "                vec = np.zeros(size)\n",
    "                vec[-1] = 1  # Assigns weight only to the last observation.\n",
    "                return vec\n",
    "            raise ValueError(\"Invalid method in conversion.\")  # Ensures an error is raised for unsupported methods.\n",
    "\n",
    "        # Extract unique (index, grain) combinations, ensuring order consistency\n",
    "        unique_combinations = df[[\"Index\", \"Grain\"]].drop_duplicates().sort_values([\"Index\", \"Grain\"])\n",
    "\n",
    "        # Get unique low-frequency index values\n",
    "        unique_indexes = unique_combinations[\"Index\"].unique()\n",
    "        n_l = len(unique_indexes)  # Number of unique low-frequency periods\n",
    "\n",
    "        # Initialize an empty conversion matrix with dimensions (low-frequency periods x total observations)\n",
    "        C = np.zeros((n_l, len(df)))\n",
    "\n",
    "        # Populate the conversion matrix by iterating through unique low-frequency indices\n",
    "        for i, idx in enumerate(unique_indexes):\n",
    "            mask = (df[\"Index\"] == idx).values  # Boolean mask for high-frequency observations corresponding to idx\n",
    "            num_valid = np.sum(mask)  # Count the number of high-frequency observations in the current period\n",
    "\n",
    "            # Assign appropriate conversion weights using the selected aggregation method\n",
    "            C[i, mask] = get_conversion_vector(num_valid, self.conversion)\n",
    "\n",
    "        return C  # Returns the constructed conversion matrix\n",
    "\n",
    "    def denton_estimation(self, y_l, X, C, h=1):\n",
    "        \"\"\"\n",
    "        Performs Denton temporal disaggregation.\n",
    "\n",
    "        This method minimizes distortions by preserving the movement of the \n",
    "        high-frequency indicator while ensuring consistency with the low-frequency data.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "            h (int, optional): Degree of differencing (0 for levels, 1 for first differences, etc.).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            n = len(X)  # Number of high-frequency observations\n",
    "\n",
    "            # Preprocess inputs to ensure proper formatting and dimensions\n",
    "            y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "            # Construct the differencing matrix (D) to compute differences in time series\n",
    "            D = np.eye(n) - np.diag(np.ones(n - 1), -1)  # First-order difference matrix (D)\n",
    "\n",
    "            # Apply differencing according to the specified degree (h)\n",
    "            # If h = 0, no differencing is applied (identity matrix is used)\n",
    "            D_h = np.linalg.matrix_power(D, h) if h > 0 else np.eye(n)\n",
    "\n",
    "            # Compute the inverse covariance matrix (Œ£_D) using the pseudoinverse\n",
    "            # This helps in controlling the smoothness of the high-frequency series\n",
    "            Sigma_D = pinv(D_h.T @ D_h)\n",
    "\n",
    "            # Compute the Denton adjustment matrix (D_matrix) \n",
    "            # This maps residual adjustments from low-frequency to high-frequency\n",
    "            D_matrix = Sigma_D @ C.T @ pinv(C @ Sigma_D @ C.T)\n",
    "\n",
    "            # Compute residuals (discrepancies between actual low-frequency values and aggregated high-frequency data)\n",
    "            u_l = y_l - C @ X\n",
    "\n",
    "            # Adjust the high-frequency series using the computed transformation matrix\n",
    "            return X + D_matrix @ u_l\n",
    "\n",
    "        except:\n",
    "            print(f\"Error in Denton estimation\")\n",
    "            return None  # Return None in case of an error\n",
    "\n",
    "\n",
    "    def chow_lin_estimation(self, y_l, X, C, rho=0.5):\n",
    "        \"\"\"\n",
    "        Performs Chow-Lin temporal disaggregation.\n",
    "\n",
    "        This method estimates high-frequency values based on a regression approach\n",
    "        with an autoregressive process.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "            rho (float, optional): Autoregressive parameter for residuals.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n = len(X)  # Number of high-frequency observations\n",
    "\n",
    "            # Preprocess inputs to ensure proper formatting and dimensions\n",
    "            y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "            # Ensure the autoregressive parameter rho is within the allowed boundaries\n",
    "            rho = np.clip(rho, self.min_rho_boundarie, self.max_rho_boundarie)\n",
    "\n",
    "            # Construct the covariance matrix (Œ£_CL) for the autoregressive process\n",
    "            # This models the dependency structure of the high-frequency residuals\n",
    "            Sigma_CL = (1 / (1 - rho**2)) * toeplitz((rho ** np.arange(n)).ravel())\n",
    "\n",
    "            # Compute the variance-covariance matrix of the aggregated series\n",
    "            Q = C @ Sigma_CL @ C.T  # Q = C * Œ£_CL * C'\n",
    "\n",
    "            # Compute the pseudo-inverse of Q to handle potential singularity issues\n",
    "            inv_Q = pinv(Q)\n",
    "\n",
    "            # Estimate the regression coefficients (Œ≤) using Generalized Least Squares (GLS)\n",
    "            # Œ≤ = (X' C' Q^-1 C X)^-1 * (X' C' Q^-1 y_l)\n",
    "            beta = solve(X.T @ C.T @ inv_Q @ C @ X, X.T @ C.T @ inv_Q @ y_l).reshape(-1, 1)\n",
    "\n",
    "            # Reshape X to ensure matrix compatibility\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "            # Compute the preliminary high-frequency estimate (p = X * Œ≤)\n",
    "            p = X @ beta\n",
    "\n",
    "            # Compute the Denton-like distribution matrix (D) that adjusts for residuals\n",
    "            # D = Œ£_CL * C' * Q^-1\n",
    "            D = Sigma_CL @ C.T @ inv_Q\n",
    "\n",
    "            # Compute the low-frequency residuals (u_l = y_l - C * p)\n",
    "            u_l = y_l.reshape(-1, 1) - C @ p\n",
    "\n",
    "            # Final high-frequency estimate by adjusting the preliminary estimate with residuals\n",
    "            return p + D @ u_l\n",
    "\n",
    "        except:\n",
    "            print(f\"Error in Chow Lin estimation\")\n",
    "            return None  # Return None in case of an error\n",
    "\n",
    "\n",
    "    def litterman_estimation(self, y_l, X, C, rho=0.5):\n",
    "        \"\"\"\n",
    "        Implements the Litterman method for temporal disaggregation.\n",
    "\n",
    "        This approach extends the Chow-Lin method by incorporating a random-walk structure \n",
    "        in the residuals, allowing for better handling of non-stationary series.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "            rho (float, optional): Autoregressive parameter.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n = len(X)  # Number of high-frequency observations\n",
    "\n",
    "            # Preprocess inputs to ensure proper formatting and dimensions\n",
    "            y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "            # Ensure the autoregressive parameter rho is within the allowed range\n",
    "            rho = np.clip(rho, self.min_rho_boundarie, self.max_rho_boundarie)\n",
    "\n",
    "            # Construct the Litterman transformation matrix (H)\n",
    "            # This incorporates the random-walk structure into the residuals\n",
    "            H = np.eye(n) - np.diag(np.ones(n - 1), -1) * rho\n",
    "\n",
    "            # Compute the inverse covariance matrix (Œ£_L), ensuring numerical stability using pinv()\n",
    "            # Œ£_L = (H' H)^-1\n",
    "            Sigma_L = pinv(H.T @ H)\n",
    "\n",
    "            # Compute the variance-covariance matrix for the aggregated series\n",
    "            Q = C @ Sigma_L @ C.T  # Q = C * Œ£_L * C'\n",
    "\n",
    "            # Compute the pseudo-inverse of Q to handle singular matrices\n",
    "            inv_Q = pinv(Q)\n",
    "\n",
    "            # Estimate the regression coefficients (Œ≤) using Generalized Least Squares (GLS)\n",
    "            # Œ≤ = (X' C' Q^-1 C X)^-1 * (X' C' Q^-1 y_l)\n",
    "            beta = solve(X.T @ C.T @ inv_Q @ C @ X, X.T @ C.T @ inv_Q @ y_l).reshape(-1, 1)\n",
    "\n",
    "            # Reshape X to ensure matrix compatibility\n",
    "            X = X.reshape(-1, 1)\n",
    "\n",
    "            # Compute the preliminary high-frequency estimate (p = X * Œ≤)\n",
    "            p = X @ beta\n",
    "\n",
    "            # Compute the adjustment matrix (D) to refine the high-frequency series\n",
    "            # D = Œ£_L * C' * Q^-1\n",
    "            D = Sigma_L @ C.T @ inv_Q\n",
    "\n",
    "            # Compute the low-frequency residuals (u_l = y_l - C * p)\n",
    "            u_l = y_l.reshape(-1, 1) - C @ p\n",
    "\n",
    "            # Final high-frequency estimate by adjusting the preliminary estimate with residuals\n",
    "            return p + D @ u_l\n",
    "\n",
    "        except:\n",
    "            print(f\"Error in Litterman estimation\")\n",
    "            return None  # Return None in case of an error\n",
    "\n",
    "\n",
    "    def fernandez_estimation(self, y_l, X, C):\n",
    "        \"\"\"\n",
    "        Uses the Fernandez method for temporal disaggregation.\n",
    "\n",
    "        This method is a special case of the Litterman approach where \n",
    "        the autoregressive parameter is set to zero, modeling residuals \n",
    "        as a simple random walk.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            n = len(X)  # Number of high-frequency observations\n",
    "\n",
    "            # Construct the first-difference operator matrix (Œî)\n",
    "            # Œî transforms the series into first differences, enforcing smoothness\n",
    "            Delta = np.eye(n) - np.diag(np.ones(n - 1), -1)\n",
    "\n",
    "            # Compute the covariance matrix (Œ£_F) for a random walk process\n",
    "            # Œ£_F = (Œî' Œî)^-1 ensures residuals follow a simple random walk\n",
    "            Sigma_F = np.linalg.inv(Delta.T @ Delta)\n",
    "\n",
    "            # Compute the variance-covariance matrix for the aggregated series\n",
    "            Q = C @ Sigma_F @ C.T  # Q = C * Œ£_F * C'\n",
    "\n",
    "            # Compute the inverse of Q to ensure numerical stability\n",
    "            inv_Q = np.linalg.inv(Q)\n",
    "\n",
    "            # Estimate the regression coefficients (Œ≤) using Generalized Least Squares (GLS)\n",
    "            # Œ≤ = (X' C' Q^-1 C X)^-1 * (X' C' Q^-1 y_l)\n",
    "            beta = solve(X.T @ C.T @ inv_Q @ C @ X, X.T @ C.T @ inv_Q @ y_l).reshape(-1, 1)\n",
    "\n",
    "            # Compute the preliminary high-frequency estimate (p = X * Œ≤)\n",
    "            p = X.reshape(-1, 1) @ beta\n",
    "\n",
    "            # Compute the adjustment matrix (D) to refine the high-frequency series\n",
    "            # D = Œ£_F * C' * Q^-1\n",
    "            D = Sigma_F @ C.T @ inv_Q\n",
    "\n",
    "            # Compute the low-frequency residuals (u_l = y_l - C * p)\n",
    "            u_l = y_l.reshape(-1, 1) - C @ p\n",
    "\n",
    "            # Final high-frequency estimate by adjusting the preliminary estimate with residuals\n",
    "            return (p + D @ u_l).flatten()\n",
    "\n",
    "        except:\n",
    "            print(f\"Error in Fernandez estimation\")\n",
    "            return None  # Return None in case of an error\n",
    "\n",
    "\n",
    "    def ols_estimation(self, y_l, X, C):\n",
    "        \"\"\"\n",
    "        Applies Ordinary Least Squares (OLS) regression for temporal disaggregation.\n",
    "\n",
    "        This method assumes a simple linear relationship between the low-frequency \n",
    "        data and the high-frequency indicators without considering autocorrelation.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure y_l and X are treated as 2D column vectors\n",
    "            # Preprocess inputs to ensure proper formatting and dimensions\n",
    "            y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "            # Aggregate the high-frequency indicator using the conversion matrix C\n",
    "            X_l = np.atleast_2d(C @ X)  # X_l represents the aggregated high-frequency data\n",
    "\n",
    "            # Compute the OLS regression coefficients (Œ≤) using the pseudo-inverse\n",
    "            # Œ≤ = (X_l' X_l)^-1 * X_l' y_l\n",
    "            beta = pinv(X_l.T @ X_l) @ X_l.T @ y_l\n",
    "\n",
    "            # Estimate the high-frequency values (≈∑ = X * Œ≤)\n",
    "            y_hat = X @ beta\n",
    "\n",
    "            # Flatten the output to return a 1D array\n",
    "            return y_hat.flatten()\n",
    "\n",
    "        except:\n",
    "            print(f\"Error in OLS estimation\")\n",
    "            return None  # Return None in case of an error\n",
    "       \n",
    "        \n",
    "    def fast_estimation(self, y_l, X, C):\n",
    "        \"\"\"\n",
    "        Provides a fast approximation of Chow-Lin estimation.\n",
    "\n",
    "        This method uses a fixed high autoregressive parameter and is computationally \n",
    "        efficient, closely replicating Denton-Cholette smoothing.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            rho = 0.9  # Fixed high autoregressive parameter for efficient computation\n",
    "            n = len(X)  # Number of high-frequency observations\n",
    "\n",
    "            # Ensure y_l and X are treated as column vectors\n",
    "            # Preprocess inputs to ensure proper formatting and dimensions\n",
    "            y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "            # Construct the transformation matrix (H)\n",
    "            # This matrix incorporates a high degree of autoregression (œÅ = 0.9)\n",
    "            H = np.eye(n) - np.diag(np.ones(n - 1), -1) * rho\n",
    "\n",
    "            # Compute the covariance matrix (Œ£_F) for the process\n",
    "            # Œ£_F = (H' H)^-1 ensures smooth transition in the estimated series\n",
    "            Sigma_F = pinv(H.T @ H)\n",
    "\n",
    "            # Compute the variance-covariance matrix for the aggregated series\n",
    "            Q = C @ Sigma_F @ C.T  # Q = C * Œ£_F * C'\n",
    "\n",
    "            # Compute the pseudo-inverse of Q to ensure numerical stability\n",
    "            inv_Q = pinv(Q)\n",
    "\n",
    "            # Estimate the regression coefficients (Œ≤) using Generalized Least Squares (GLS)\n",
    "            # Œ≤ = (X' C' Q^-1 C X)^-1 * (X' C' Q^-1 y_l)\n",
    "            beta = solve(X.T @ C.T @ inv_Q @ C @ X, X.T @ C.T @ inv_Q @ y_l)\n",
    "\n",
    "            # Compute the preliminary high-frequency estimate (p = X * Œ≤)\n",
    "            p = X @ beta\n",
    "\n",
    "            # Compute the adjustment matrix (D) to refine the high-frequency series\n",
    "            # D = Œ£_F * C' * Q^-1\n",
    "            D = Sigma_F @ C.T @ inv_Q\n",
    "\n",
    "            # Compute the low-frequency residuals (u_l = y_l - C * p)\n",
    "            u_l = y_l - C @ p\n",
    "\n",
    "            # Final high-frequency estimate by adjusting the preliminary estimate with residuals\n",
    "            y_hat = p + D @ u_l\n",
    "\n",
    "            # Flatten the output to return a 1D array\n",
    "            return y_hat.flatten()\n",
    "\n",
    "        except:\n",
    "            print(f\"Error in Fast estimation\")\n",
    "            return None  # Return None in case of an error\n",
    "\n",
    "\n",
    "    def power_matrix_calculation(self, n):\n",
    "        \"\"\"\n",
    "        Computes a power matrix used in autoregressive modeling.\n",
    "\n",
    "        This matrix captures dependencies between different time periods \n",
    "        to model the persistence of the series.\n",
    "\n",
    "        Parameters:\n",
    "            n (int): Number of time periods.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Power matrix for autoregressive modeling.\n",
    "        \"\"\"\n",
    "        # Generate a matrix where each entry (i, j) represents |i - j|\n",
    "        # This encodes the absolute distance between time periods\n",
    "        # and is useful in modeling autoregressive dependencies.\n",
    "        return np.abs(np.subtract.outer(np.arange(n), np.arange(n)))\n",
    "\n",
    "\n",
    "    def q_calculation(self, rho, pm):\n",
    "        \"\"\"\n",
    "        Computes the covariance matrix for an autoregressive process.\n",
    "\n",
    "        This matrix is used in regression-based disaggregation methods \n",
    "        to model the correlation between observations.\n",
    "\n",
    "        Parameters:\n",
    "            rho (float): Autoregressive parameter.\n",
    "            pm (np.ndarray): Power matrix representing time dependencies.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Covariance matrix for the autoregressive process.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-6  # Small constant to prevent division by zero or instability\n",
    "\n",
    "        # Ensure rho is within the valid range to maintain numerical stability\n",
    "        rho = np.clip(rho, self.min_rho_boundarie, self.max_rho_boundarie)\n",
    "\n",
    "        # Compute the scaling factor for the covariance matrix\n",
    "        # This ensures proper normalization to prevent over-scaling of the model\n",
    "        factor = 1 / (1 - rho**2 + epsilon)\n",
    "\n",
    "        # Compute the covariance matrix using the power matrix (pm)\n",
    "        # Each entry represents rho raised to the power of the absolute difference in time steps\n",
    "        Q = factor * (rho ** pm)\n",
    "\n",
    "        return Q\n",
    "\n",
    "        \n",
    "    def q_lit_calculation(self, X, rho=0):\n",
    "        \"\"\"\n",
    "        Computes the pseudo-variance-covariance matrix for the Litterman method.\n",
    "\n",
    "        This matrix incorporates an autoregressive structure if specified.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            rho (float, optional): Autoregressive parameter. Defaults to 0 (no autoregression).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Pseudo-variance-covariance matrix.\n",
    "        \"\"\"\n",
    "        n = X.shape[0]  # Number of high-frequency observations\n",
    "        epsilon = 1e-8  # Small constant to improve numerical stability\n",
    "\n",
    "        # Construct the transformation matrix (H) incorporating the autoregressive parameter (œÅ)\n",
    "        # H introduces the autoregressive component in the variance structure\n",
    "        H = np.eye(n) - np.diag(np.ones(n - 1), -1) * rho\n",
    "\n",
    "        # Construct the first-difference operator matrix (D)\n",
    "        # D applies a first-difference transformation, ensuring smoothness in estimates\n",
    "        D = np.eye(n) - np.diag(np.ones(n - 1), -1)\n",
    "\n",
    "        # Compute the pseudo-variance-covariance matrix (Q_Lit)\n",
    "        # This matrix captures dependencies in the autoregressive process\n",
    "        Q_Lit = D.T @ H.T @ H @ D\n",
    "\n",
    "        try:\n",
    "            # Compute the inverse of Q_Lit with regularization (adds Œµ * I to avoid singularity)\n",
    "            Q_Lit_inv = np.linalg.inv(Q_Lit + np.eye(n) * epsilon)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # If the matrix is singular, use the Moore-Penrose pseudo-inverse\n",
    "            Q_Lit_inv = np.linalg.pinv(Q_Lit)\n",
    "\n",
    "        return Q_Lit_inv\n",
    "\n",
    "\n",
    "    def rho_optimization(self, y_l, X, C, method=\"maxlog\"):\n",
    "        \"\"\"\n",
    "        Finds the optimal autoregressive parameter (rho).\n",
    "\n",
    "        This is done by maximizing the likelihood function or minimizing \n",
    "        the residual sum of squares, which is crucial for Chow-Lin and Litterman methods.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "            method (str, optional): Optimization criterion. Options: \"maxlog\" (maximize log-likelihood) \n",
    "                                    or \"minrss\" (minimize residual sum of squares).\n",
    "\n",
    "        Returns:\n",
    "            float: Optimal autoregressive parameter rho.\n",
    "        \"\"\"\n",
    "        # Compute the aggregated high-frequency series using the conversion matrix\n",
    "        X_l = np.atleast_2d(C @ X)\n",
    "\n",
    "        # Compute the power matrix, which captures time dependencies\n",
    "        pm = self.power_matrix_calculation(X.shape[0])\n",
    "\n",
    "        # Ensure input dimensions are properly formatted before optimization\n",
    "        y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "        def objective(rho):\n",
    "            \"\"\"\n",
    "            Defines the optimization objective function for rho.\n",
    "\n",
    "            The function either maximizes the log-likelihood (maxlog) \n",
    "            or minimizes the residual sum of squares (minrss).\n",
    "            \"\"\"\n",
    "            # Ensure rho is within the specified boundaries\n",
    "            if not (self.min_rho_boundarie < rho < self.max_rho_boundarie):\n",
    "                return np.inf  # Return infinity to prevent invalid rho values\n",
    "\n",
    "            # Compute the covariance matrix Q for the autoregressive process\n",
    "            Q = self.q_calculation(rho, pm)\n",
    "\n",
    "            # Compute the variance-covariance matrix for the aggregated series\n",
    "            vcov = C @ Q @ C.T\n",
    "\n",
    "            # Compute the inverse of vcov, adding a small regularization term for stability\n",
    "            inv_vcov = pinv(vcov + np.eye(vcov.shape[0]) * 1e-8)\n",
    "\n",
    "            # Ensure that the dimensions match for matrix operations\n",
    "            if X_l.shape[0] != inv_vcov.shape[0]:\n",
    "                return np.inf  # Return infinity to prevent errors\n",
    "\n",
    "            # Compute (X' C' inv_vcov C X), ensuring it's a square matrix\n",
    "            XTX = X_l.T @ inv_vcov @ X_l\n",
    "            if XTX.shape[0] != XTX.shape[1]:\n",
    "                return np.inf  # Return infinity if matrix inversion is not feasible\n",
    "\n",
    "            # Estimate regression coefficients (Œ≤) using the Generalized Least Squares (GLS) formula\n",
    "            beta = pinv(XTX) @ X_l.T @ inv_vcov @ y_l\n",
    "\n",
    "            # Compute residuals (difference between observed and estimated values)\n",
    "            u_l = y_l - X_l @ beta\n",
    "\n",
    "            # Choose the optimization method: log-likelihood maximization or residual minimization\n",
    "            if method == \"maxlog\":\n",
    "                # Maximize log-likelihood: minimize its negative\n",
    "                return -(-0.5 * (np.log(np.abs(np.linalg.det(vcov)) + 1e-8) + u_l.T @ inv_vcov @ u_l))\n",
    "            elif method == \"minrss\":\n",
    "                # Minimize the residual sum of squares\n",
    "                return u_l.T @ inv_vcov @ u_l\n",
    "            else:\n",
    "                raise ValueError(\"Invalid method for rho calculation\")\n",
    "\n",
    "        # Perform bounded scalar optimization within the defined range of rho\n",
    "        opt_result = minimize_scalar(objective, bounds=(self.min_rho_boundarie, self.max_rho_boundarie), method=\"bounded\")\n",
    "\n",
    "        # Return the optimal rho value\n",
    "        return opt_result.x\n",
    "\n",
    "    \n",
    "    def litterman_opt_estimation(self, y_l, X, C):\n",
    "        \"\"\"\n",
    "        Implements the optimized Litterman method.\n",
    "\n",
    "        This method estimates the best autoregressive parameter before performing \n",
    "        disaggregation, refining the standard Litterman approach for better accuracy.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Preprocess inputs to ensure proper formatting and dimensions\n",
    "            y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "            # Validate that the number of columns in C matches the number of rows in X\n",
    "            if C.shape[1] != X.shape[0]:\n",
    "                return None  # Return None if dimensions are incompatible\n",
    "\n",
    "            # Compute the aggregated high-frequency series using the conversion matrix\n",
    "            X_l = np.atleast_2d(C @ X)\n",
    "\n",
    "            # Optimize the autoregressive parameter (rho) by minimizing residual sum of squares (minrss)\n",
    "            rho_opt = self.rho_optimization(y_l, X, C, method=\"minrss\")\n",
    "\n",
    "            # Compute the variance-covariance matrix (Q_Lit) for the optimized Litterman method\n",
    "            Q_Lit = self.q_lit_calculation(X, rho_opt)\n",
    "\n",
    "            # Compute the variance-covariance matrix for the aggregated series\n",
    "            vcov = C @ Q_Lit @ C.T\n",
    "\n",
    "            # Compute the pseudo-inverse of vcov, adding a small regularization term for numerical stability\n",
    "            inv_vcov = pinv(vcov + np.eye(vcov.shape[0]) * 1e-8)\n",
    "\n",
    "            # Validate dimensions to prevent computational errors\n",
    "            if X_l.shape[0] != inv_vcov.shape[0]:\n",
    "                return None  # Return None if dimensions mismatch\n",
    "\n",
    "            # Compute (X' C' inv_vcov C X), ensuring it is a square matrix before inversion\n",
    "            XTX = X_l.T @ inv_vcov @ X_l\n",
    "            if XTX.shape[0] != XTX.shape[1]:\n",
    "                return None  # Return None if matrix is not square\n",
    "\n",
    "            # Estimate regression coefficients (Œ≤) using Generalized Least Squares (GLS)\n",
    "            # Œ≤ = (X' C' Q^-1 C X)^-1 * (X' C' Q^-1 y_l)\n",
    "            beta = pinv(XTX) @ X_l.T @ inv_vcov @ y_l\n",
    "\n",
    "            # Compute the preliminary high-frequency estimate (p = X * Œ≤)\n",
    "            p = X @ beta\n",
    "\n",
    "            # Compute the adjustment matrix (D) to refine the high-frequency series\n",
    "            # D = Q_Lit * C' * Q^-1\n",
    "            D = Q_Lit @ C.T @ inv_vcov\n",
    "\n",
    "            # Compute the low-frequency residuals (u_l = y_l - C * p)\n",
    "            u_l = y_l - C @ p\n",
    "\n",
    "            # Final high-frequency estimate by adjusting the preliminary estimate with residuals\n",
    "            y_hat = p + D @ u_l\n",
    "\n",
    "            # Flatten the output to return a 1D array\n",
    "            return y_hat.flatten()\n",
    "        except:\n",
    "            print(\"Error in optimized Litterman\")\n",
    "\n",
    "\n",
    "    def chow_lin_opt_estimation(self, y_l, X, C):\n",
    "        \"\"\"\n",
    "        Implements the optimized Chow-Lin method.\n",
    "\n",
    "        This method estimates the best autoregressive parameter before performing \n",
    "        disaggregation, improving accuracy by tuning the autoregressive component.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Preprocess inputs to ensure proper formatting and dimensions\n",
    "            y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "            # Validate that the number of columns in C matches the number of rows in X\n",
    "            if C.shape[1] != X.shape[0]:\n",
    "                return None  # Return None if dimensions are incompatible\n",
    "\n",
    "            # Compute the aggregated high-frequency series using the conversion matrix\n",
    "            X_l = np.atleast_2d(C @ X)\n",
    "\n",
    "            # Optimize the autoregressive parameter (rho) by maximizing the log-likelihood (maxlog)\n",
    "            rho_opt = self.rho_optimization(y_l, X, C, method=\"maxlog\")\n",
    "\n",
    "            # Number of high-frequency observations\n",
    "            n = X.shape[0]\n",
    "\n",
    "            # Construct the autoregressive covariance matrix (Œ£_CL)\n",
    "            # This models the dependency structure of the high-frequency residuals\n",
    "            Sigma_CL = (1 / (1 - rho_opt**2)) * toeplitz(np.ravel(rho_opt ** np.arange(n)))\n",
    "\n",
    "            # Compute the variance-covariance matrix for the aggregated series\n",
    "            Q = C @ Sigma_CL @ C.T\n",
    "\n",
    "            # Ensure Q is a square matrix before inversion\n",
    "            if Q.shape[0] != Q.shape[1]:\n",
    "                return None  # Return None if dimensions are incorrect\n",
    "\n",
    "            # Compute the pseudo-inverse of Q, adding a small regularization term for numerical stability\n",
    "            inv_Q = pinv(Q + np.eye(Q.shape[0]) * 1e-8)\n",
    "\n",
    "            # Validate dimensions to prevent computational errors\n",
    "            if X_l.shape[0] != inv_Q.shape[0]:\n",
    "                return None  # Return None if dimensions mismatch\n",
    "\n",
    "            # Compute (X' C' inv_Q C X), ensuring it is a square matrix before inversion\n",
    "            XTX = X_l.T @ inv_Q @ X_l\n",
    "            if XTX.shape[0] != XTX.shape[1]:\n",
    "                return None  # Return None if matrix is not square\n",
    "\n",
    "            # Estimate regression coefficients (Œ≤) using Generalized Least Squares (GLS)\n",
    "            # Œ≤ = (X' C' Q^-1 C X)^-1 * (X' C' Q^-1 y_l)\n",
    "            beta = pinv(XTX) @ X_l.T @ inv_Q @ y_l\n",
    "\n",
    "            # Compute the preliminary high-frequency estimate (p = X * Œ≤)\n",
    "            p = X @ beta\n",
    "\n",
    "            # Compute the adjustment matrix (D) to refine the high-frequency series\n",
    "            # D = Œ£_CL * C' * Q^-1\n",
    "            D = Sigma_CL @ C.T @ inv_Q\n",
    "\n",
    "            # Compute the low-frequency residuals (u_l = y_l - C * p)\n",
    "            u_l = y_l - C @ p\n",
    "\n",
    "            # Final high-frequency estimate by adjusting the preliminary estimate with residuals\n",
    "            y_hat = p + D @ u_l\n",
    "\n",
    "            # Flatten the output to return a 1D array\n",
    "            return y_hat.flatten()\n",
    "        except:\n",
    "            print(\"Error in optimized Litterman\")\n",
    "\n",
    "    \n",
    "    def preprocess_inputs(self, y_l, X, C):\n",
    "        \"\"\"\n",
    "        Preprocesses inputs for temporal disaggregation methods.\n",
    "\n",
    "        This function ensures the correct shape and format of the input data.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Processed (y_l, X, C) as numpy arrays with the correct dimensions.\n",
    "        \"\"\"\n",
    "        # Ensure y_l and X are treated as 2D column vectors\n",
    "        y_l = np.atleast_2d(y_l).reshape(-1, 1)\n",
    "        X = np.atleast_2d(X).reshape(-1, 1)\n",
    "\n",
    "        # Validate that the number of rows in C matches the number of observations in y_l\n",
    "        if C.shape[0] != y_l.shape[0]:\n",
    "            raise ValueError(f\"Shape mismatch: C.shape[0] ({C.shape[0]}) != y_l.shape[0] ({y_l.shape[0]})\")\n",
    "\n",
    "        # Validate that the number of columns in C matches the number of rows in X\n",
    "        if C.shape[1] != X.shape[0]:\n",
    "            raise ValueError(f\"Shape mismatch: C.shape[1] ({C.shape[1]}) != X.shape[0] ({X.shape[0]})\")\n",
    "\n",
    "        # Return the processed inputs with corrected dimensions\n",
    "        return y_l, X, C\n",
    "    \n",
    "\n",
    "    def chow_lin_minrss_ecotrim(self, y_l, X, C, rho=0.75):\n",
    "        \"\"\"\n",
    "        Implements the Chow-Lin method with RSS minimization (Ecotrim).\n",
    "\n",
    "        This method estimates high-frequency values by minimizing the \n",
    "        residual sum of squares (RSS) while preserving correlation structure.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "            rho (float): Autoregressive parameter.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        # Preprocess inputs to ensure proper formatting and dimensions\n",
    "        y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "                                               \n",
    "        n = X.shape[0]  # Number of high-frequency observations\n",
    "\n",
    "        # Ensure rho is within valid boundaries\n",
    "        rho = np.clip(rho, self.min_rho_boundarie, self.max_rho_boundarie)\n",
    "\n",
    "        # Compute the correlation matrix R (instead of covariance matrix)\n",
    "        # Toeplitz structure models time dependency with autoregressive correlation\n",
    "        R = toeplitz(rho ** np.arange(n))\n",
    "\n",
    "        # Compute the aggregated variance-covariance matrix\n",
    "        Q = C @ R @ C.T  # Q = C * R * C'\n",
    "\n",
    "        # Compute the pseudo-inverse for numerical stability\n",
    "        inv_Q = pinv(Q + np.eye(Q.shape[0]) * 1e-8)\n",
    "\n",
    "        # Generalized Least Squares (GLS) estimation of beta coefficients\n",
    "        beta = pinv(X.T @ C.T @ inv_Q @ C @ X) @ X.T @ C.T @ inv_Q @ y_l\n",
    "\n",
    "        # Compute the preliminary high-frequency estimate (p = X * Œ≤)\n",
    "        p = X @ beta\n",
    "\n",
    "        # Compute the adjustment matrix (D) to refine the high-frequency series\n",
    "        # D = R * C' * Q^-1\n",
    "        D = R @ C.T @ inv_Q\n",
    "\n",
    "        # Compute the low-frequency residuals (u_l = y_l - C * p)\n",
    "        u_l = y_l - C @ p\n",
    "\n",
    "        # Final high-frequency estimate by adjusting the preliminary estimate with residuals\n",
    "        return p + D @ u_l\n",
    "    \n",
    "    def chow_lin_minrss_quilis(self, y_l, X, C, rho=0.15):\n",
    "        \"\"\"\n",
    "        Implements the Chow-Lin method with RSS minimization (Quilis approach).\n",
    "\n",
    "        This method estimates high-frequency values by minimizing the \n",
    "        residual sum of squares (RSS) while scaling the correlation matrix.\n",
    "\n",
    "        Parameters:\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "            rho (float): Autoregressive parameter.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        # Preprocess inputs to ensure proper formatting and dimensions\n",
    "        y_l, X, C = self.preprocess_inputs(y_l, X, C)\n",
    "\n",
    "        n = X.shape[0]  # Number of high-frequency observations\n",
    "\n",
    "        # Ensure rho is within valid boundaries\n",
    "        rho = np.clip(rho, self.min_rho_boundarie, self.max_rho_boundarie)\n",
    "\n",
    "        # Compute the scaled correlation matrix R\n",
    "        # Unlike Ecotrim, Quilis scales the matrix with (1 / (1 - rho^2))\n",
    "        epsilon = 1e-6 \n",
    "        R = (1 / (1 - (rho + epsilon)**2)) * toeplitz(rho ** np.arange(n))\n",
    "\n",
    "        # Compute the aggregated variance-covariance matrix\n",
    "        Q = C @ R @ C.T  # Q = C * R * C'\n",
    "\n",
    "        # Compute the pseudo-inverse for numerical stability\n",
    "        inv_Q = pinv(Q + np.eye(Q.shape[0]) * 1e-8)\n",
    "\n",
    "        # Generalized Least Squares (GLS) estimation of beta coefficients\n",
    "        beta = pinv(X.T @ C.T @ inv_Q @ C @ X) @ X.T @ C.T @ inv_Q @ y_l\n",
    "\n",
    "        # Compute the preliminary high-frequency estimate (p = X * Œ≤)\n",
    "        p = X @ beta\n",
    "\n",
    "        # Compute the adjustment matrix (D) to refine the high-frequency series\n",
    "        # D = R * C' * Q^-1\n",
    "        D = R @ C.T @ inv_Q\n",
    "\n",
    "        # Compute the low-frequency residuals (u_l = y_l - C * p)\n",
    "        u_l = y_l - C @ p\n",
    "\n",
    "        # Final high-frequency estimate by adjusting the preliminary estimate with residuals\n",
    "        return p + D @ u_l\n",
    "\n",
    "    def predict(self, df, method, **kwargs):\n",
    "        \"\"\"\n",
    "        General interface for performing temporal disaggregation.\n",
    "\n",
    "        Selects and applies the appropriate estimation method.\n",
    "\n",
    "        Parameters:\n",
    "            method (str): The disaggregation method to use.\n",
    "            y_l (np.ndarray): Low-frequency time series.\n",
    "            X (np.ndarray): High-frequency indicator series.\n",
    "            C (np.ndarray): Conversion matrix.\n",
    "            **kwargs: Additional parameters for specific methods.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The estimated high-frequency series.\n",
    "        \"\"\"\n",
    "        # Dictionary of available disaggregation methods\n",
    "        all_methods = {\n",
    "            \"ols\": self.ols_estimation,\n",
    "            \"denton\": self.denton_estimation,\n",
    "            \"chow-lin\": self.chow_lin_estimation,\n",
    "            \"litterman\": self.litterman_estimation,\n",
    "            \"fernandez\": self.fernandez_estimation,\n",
    "            \"fast\": self.fast_estimation,\n",
    "            \"chow-lin-opt\": self.chow_lin_opt_estimation,\n",
    "            \"litterman-opt\": self.litterman_opt_estimation,\n",
    "            \"chow-lin-ecotrim\": self.chow_lin_minrss_ecotrim,\n",
    "            \"chow-lin-quilis\": self.chow_lin_minrss_quilis,\n",
    "        }\n",
    "\n",
    "        df_predicted = df.copy()\n",
    "\n",
    "        # Build the conversion matrix\n",
    "        C = self.build_conversion_matrix(df_predicted)\n",
    "\n",
    "        # Extract low-frequency series (one observation per low-frequency period)\n",
    "        y_l = df_predicted.groupby(\"Index\")[\"y\"].first().values\n",
    "\n",
    "        # Extract high-frequency indicator series\n",
    "        X = df_predicted[\"X\"].values\n",
    "\n",
    "        # Ensure the method is valid\n",
    "        if method not in all_methods:\n",
    "            raise ValueError(f\"Method '{method}' is not supported. Available methods: {list(all_methods.keys())}\")\n",
    "\n",
    "        # Ensure input arrays are column vectors\n",
    "        y_l = np.atleast_2d(y_l).reshape(-1, 1)\n",
    "        X = np.atleast_2d(X).reshape(-1, 1)\n",
    "\n",
    "        # Validate matrix dimensions before proceeding\n",
    "        if C.shape[1] != X.shape[0]:\n",
    "            raise ValueError(f\"Shape mismatch: C.shape[1] ({C.shape[1]}) != X.shape[0] ({X.shape[0]})\")\n",
    "        if C.shape[0] != y_l.shape[0]:\n",
    "            raise ValueError(f\"Shape mismatch: C.shape[0] ({C.shape[0]}) != y_l.shape[0] ({y_l.shape[0]})\")\n",
    "\n",
    "        # Define the list of valid keyword arguments that methods can accept\n",
    "        valid_args = [\"h\", \"rho\", \"alpha\", \"weights\"]\n",
    "\n",
    "        # Filter only the valid arguments to pass to the method\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_args}\n",
    "\n",
    "        try:\n",
    "            # Execute the selected disaggregation method and return the estimated series\n",
    "            y_hat = all_methods[method](y_l, X, C, **filtered_kwargs)\n",
    "    \n",
    "        except:\n",
    "            try:\n",
    "                # Execute the selected disaggregation method (first backup)\n",
    "                warnings.warn(\"Error in disaggregation. FAST used as default method\", UserWarning)\n",
    "                y_hat = all_methods[\"fast\"](y_l, X, C, **filtered_kwargs)\n",
    "             \n",
    "            except:\n",
    "                # Execute the selected disaggregation method (second backup)\n",
    "                warnings.warn(\"Error in disaggregation. OLS used as default method\", UserWarning)\n",
    "                y_hat = all_methods[\"ols\"](y_l, X, C, **filtered_kwargs)\n",
    "\n",
    "        df_predicted[\"y_hat\"] = y_hat\n",
    "\n",
    "        if self.apply_adjustment:\n",
    "            df_predicted = self.adjust_negative_values(df_predicted)\n",
    "\n",
    "        return df_predicted\n",
    "    \n",
    "\n",
    "    def adjust_negative_values(self, df):\n",
    "        \"\"\"\n",
    "        Adjusts negative values in the predicted high-frequency series while preserving aggregation constraints.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The dataframe containing the following columns:\n",
    "                            ['Index', 'Grain', 'X', 'y', 'y_hat'].\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The adjusted dataframe with non-negative values in 'y_hat'.\n",
    "        \"\"\"\n",
    "        df_adjusted = df.copy()\n",
    "        df_adjusted[\"y_hat_adj\"] = df_adjusted[\"y_hat\"].copy()\n",
    "        \n",
    "        # Identificar los √≠ndices con valores negativos en y_hat\n",
    "        negative_indexes = df_adjusted[df_adjusted[\"y_hat\"] < 0][\"Index\"].unique()\n",
    "        \n",
    "        for index in negative_indexes:\n",
    "            group = df_adjusted[df_adjusted[\"Index\"] == index].reset_index(drop=True)\n",
    "            y_hat = group[\"y_hat\"].values\n",
    "\n",
    "            # Si no hay valores negativos, continuar con el siguiente grupo\n",
    "            if (y_hat >= 0).all():\n",
    "                continue\n",
    "            \n",
    "            if self.conversion in [\"sum\", \"average\"]:\n",
    "                # Sumar todas las diferencias negativas para crear un factor de ajuste\n",
    "                negative_sum = np.abs(y_hat[y_hat < 0].sum())\n",
    "                \n",
    "                # Obtener valores positivos y su suma\n",
    "                positive_values = y_hat[y_hat > 0]\n",
    "                positive_sum = positive_values.sum()\n",
    "\n",
    "                if positive_sum > 0:\n",
    "                    # Calcular las participaciones relativas de los valores positivos\n",
    "                    weights = positive_values / positive_sum\n",
    "\n",
    "                    # Aplicar ajuste proporcionalmente\n",
    "                    y_hat[y_hat > 0] -= negative_sum * weights\n",
    "                    \n",
    "                    # Ajustar los valores negativos a un peque√±o valor positivo si se requiere\n",
    "                    y_hat[y_hat < 0] = 0\n",
    "\n",
    "                else:\n",
    "                    # Si no hay valores positivos, distribuir de manera uniforme\n",
    "                    y_hat[:] = negative_sum / len(y_hat)\n",
    "\n",
    "            elif self.conversion == \"first\":\n",
    "                # Mantener el primer valor fijo\n",
    "                first_value = y_hat[0]\n",
    "                remaining_values = y_hat[1:]\n",
    "\n",
    "                if remaining_values.sum() < 0:\n",
    "                    remaining_values[:] = 0\n",
    "                else:\n",
    "                    negative_sum = np.abs(remaining_values[remaining_values < 0].sum())\n",
    "                    positive_values = remaining_values[remaining_values > 0]\n",
    "                    positive_sum = positive_values.sum()\n",
    "\n",
    "                    if positive_sum > 0:\n",
    "                        weights = positive_values / positive_sum\n",
    "                        remaining_values[remaining_values > 0] -= negative_sum * weights\n",
    "                    \n",
    "                    remaining_values[remaining_values < 0] = 0\n",
    "                \n",
    "                y_hat[1:] = remaining_values\n",
    "                y_hat[0] = first_value\n",
    "\n",
    "            elif self.conversion == \"last\":\n",
    "                # Mantener el √∫ltimo valor fijo\n",
    "                last_value = y_hat[-1]\n",
    "                remaining_values = y_hat[:-1]\n",
    "\n",
    "                if remaining_values.sum() < 0:\n",
    "                    remaining_values[:] = 0\n",
    "                else:\n",
    "                    negative_sum = np.abs(remaining_values[remaining_values < 0].sum())\n",
    "                    positive_values = remaining_values[remaining_values > 0]\n",
    "                    positive_sum = positive_values.sum()\n",
    "\n",
    "                    if positive_sum > 0:\n",
    "                        weights = positive_values / positive_sum\n",
    "                        remaining_values[remaining_values > 0] -= negative_sum * weights\n",
    "                    \n",
    "                    remaining_values[remaining_values < 0] = 0\n",
    "                \n",
    "                y_hat[:-1] = remaining_values\n",
    "                y_hat[-1] = last_value\n",
    "            \n",
    "            # Reasignar los valores ajustados\n",
    "            df_adjusted.loc[df_adjusted[\"Index\"] == index, \"y_hat_adj\"] = y_hat\n",
    "\n",
    "        return df_adjusted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia de la clase con conversi√≥n por promedio\n",
    "td = TemporalDisaggregation(conversion=\"average\", apply_adjustment = False)\n",
    "\n",
    "# Realizar la predicci√≥n en un solo paso\n",
    "df_res = td.predict(merged_series, method=\"chow-lin-opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Grain</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>150.0</td>\n",
       "      <td>146.773631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>100.909091</td>\n",
       "      <td>150.0</td>\n",
       "      <td>147.965087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>101.818182</td>\n",
       "      <td>150.0</td>\n",
       "      <td>148.975339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>102.727273</td>\n",
       "      <td>150.0</td>\n",
       "      <td>149.804334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>103.636364</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.451985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2023</td>\n",
       "      <td>8</td>\n",
       "      <td>346.363636</td>\n",
       "      <td>500.0</td>\n",
       "      <td>486.074653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>347.272727</td>\n",
       "      <td>500.0</td>\n",
       "      <td>487.332949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>2023</td>\n",
       "      <td>10</td>\n",
       "      <td>348.181818</td>\n",
       "      <td>500.0</td>\n",
       "      <td>488.578955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>349.090909</td>\n",
       "      <td>500.0</td>\n",
       "      <td>489.812668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>500.0</td>\n",
       "      <td>491.034080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index  Grain           X      y       y_hat\n",
       "0     2001      1  100.000000  150.0  146.773631\n",
       "1     2001      2  100.909091  150.0  147.965087\n",
       "2     2001      3  101.818182  150.0  148.975339\n",
       "3     2001      4  102.727273  150.0  149.804334\n",
       "4     2001      5  103.636364  150.0  150.451985\n",
       "..     ...    ...         ...    ...         ...\n",
       "271   2023      8  346.363636  500.0  486.074653\n",
       "272   2023      9  347.272727  500.0  487.332949\n",
       "273   2023     10  348.181818  500.0  488.578955\n",
       "274   2023     11  349.090909  500.0  489.812668\n",
       "275   2023     12  350.000000  500.0  491.034080\n",
       "\n",
       "[276 rows x 5 columns]"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Grain</th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y_hat_adj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>41.702200</td>\n",
       "      <td>988.861089</td>\n",
       "      <td>1003.046507</td>\n",
       "      <td>1003.046507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>72.032449</td>\n",
       "      <td>988.861089</td>\n",
       "      <td>1034.655128</td>\n",
       "      <td>1034.655128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.011437</td>\n",
       "      <td>988.861089</td>\n",
       "      <td>952.220090</td>\n",
       "      <td>952.220090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>4</td>\n",
       "      <td>30.233257</td>\n",
       "      <td>988.861089</td>\n",
       "      <td>965.522630</td>\n",
       "      <td>965.522630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>14.675589</td>\n",
       "      <td>748.165654</td>\n",
       "      <td>922.785151</td>\n",
       "      <td>922.785151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>9.233859</td>\n",
       "      <td>748.165654</td>\n",
       "      <td>834.762690</td>\n",
       "      <td>834.762690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>18.626021</td>\n",
       "      <td>748.165654</td>\n",
       "      <td>706.284504</td>\n",
       "      <td>706.284504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>34.556073</td>\n",
       "      <td>748.165654</td>\n",
       "      <td>528.830263</td>\n",
       "      <td>528.830263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>39.676747</td>\n",
       "      <td>280.443992</td>\n",
       "      <td>284.593228</td>\n",
       "      <td>284.593228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>53.881673</td>\n",
       "      <td>280.443992</td>\n",
       "      <td>193.063419</td>\n",
       "      <td>193.063419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2002</td>\n",
       "      <td>3</td>\n",
       "      <td>41.919451</td>\n",
       "      <td>280.443992</td>\n",
       "      <td>218.084823</td>\n",
       "      <td>218.084823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2002</td>\n",
       "      <td>4</td>\n",
       "      <td>68.521950</td>\n",
       "      <td>280.443992</td>\n",
       "      <td>426.034522</td>\n",
       "      <td>426.034522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>20.445225</td>\n",
       "      <td>789.279328</td>\n",
       "      <td>700.815240</td>\n",
       "      <td>700.815240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>87.811744</td>\n",
       "      <td>789.279328</td>\n",
       "      <td>925.029879</td>\n",
       "      <td>925.029879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2003</td>\n",
       "      <td>3</td>\n",
       "      <td>2.738759</td>\n",
       "      <td>789.279328</td>\n",
       "      <td>824.018509</td>\n",
       "      <td>824.018509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>67.046751</td>\n",
       "      <td>789.279328</td>\n",
       "      <td>707.253660</td>\n",
       "      <td>707.253660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>41.730480</td>\n",
       "      <td>103.226007</td>\n",
       "      <td>329.651025</td>\n",
       "      <td>305.039191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2004</td>\n",
       "      <td>2</td>\n",
       "      <td>55.868983</td>\n",
       "      <td>103.226007</td>\n",
       "      <td>116.567842</td>\n",
       "      <td>107.864855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2004</td>\n",
       "      <td>3</td>\n",
       "      <td>14.038694</td>\n",
       "      <td>103.226007</td>\n",
       "      <td>-29.862173</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2004</td>\n",
       "      <td>4</td>\n",
       "      <td>19.810149</td>\n",
       "      <td>103.226007</td>\n",
       "      <td>-3.452649</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>80.074457</td>\n",
       "      <td>447.893526</td>\n",
       "      <td>202.864336</td>\n",
       "      <td>202.864336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2005</td>\n",
       "      <td>2</td>\n",
       "      <td>96.826158</td>\n",
       "      <td>447.893526</td>\n",
       "      <td>380.244889</td>\n",
       "      <td>380.244889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>31.342418</td>\n",
       "      <td>447.893526</td>\n",
       "      <td>489.000452</td>\n",
       "      <td>489.000452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>69.232262</td>\n",
       "      <td>447.893526</td>\n",
       "      <td>719.464430</td>\n",
       "      <td>719.464430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>87.638915</td>\n",
       "      <td>908.595503</td>\n",
       "      <td>945.684065</td>\n",
       "      <td>945.684065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2006</td>\n",
       "      <td>2</td>\n",
       "      <td>89.460666</td>\n",
       "      <td>908.595503</td>\n",
       "      <td>1032.165034</td>\n",
       "      <td>1032.165034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2006</td>\n",
       "      <td>3</td>\n",
       "      <td>8.504421</td>\n",
       "      <td>908.595503</td>\n",
       "      <td>911.044299</td>\n",
       "      <td>911.044299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2006</td>\n",
       "      <td>4</td>\n",
       "      <td>3.905478</td>\n",
       "      <td>908.595503</td>\n",
       "      <td>745.488595</td>\n",
       "      <td>745.488595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>16.983042</td>\n",
       "      <td>293.614148</td>\n",
       "      <td>475.312865</td>\n",
       "      <td>475.312865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>87.814250</td>\n",
       "      <td>293.614148</td>\n",
       "      <td>352.729447</td>\n",
       "      <td>352.729447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2007</td>\n",
       "      <td>3</td>\n",
       "      <td>9.834683</td>\n",
       "      <td>293.614148</td>\n",
       "      <td>165.916212</td>\n",
       "      <td>165.916212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2007</td>\n",
       "      <td>4</td>\n",
       "      <td>42.110763</td>\n",
       "      <td>293.614148</td>\n",
       "      <td>180.498083</td>\n",
       "      <td>180.498083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>95.788953</td>\n",
       "      <td>287.775339</td>\n",
       "      <td>305.366540</td>\n",
       "      <td>305.366540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2008</td>\n",
       "      <td>2</td>\n",
       "      <td>53.316528</td>\n",
       "      <td>287.775339</td>\n",
       "      <td>296.380042</td>\n",
       "      <td>296.380042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>69.187711</td>\n",
       "      <td>287.775339</td>\n",
       "      <td>311.953908</td>\n",
       "      <td>311.953908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>31.551563</td>\n",
       "      <td>287.775339</td>\n",
       "      <td>237.400858</td>\n",
       "      <td>237.400858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>68.650093</td>\n",
       "      <td>130.028572</td>\n",
       "      <td>204.211173</td>\n",
       "      <td>204.211173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2009</td>\n",
       "      <td>2</td>\n",
       "      <td>83.462567</td>\n",
       "      <td>130.028572</td>\n",
       "      <td>165.917351</td>\n",
       "      <td>165.917351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>1.828828</td>\n",
       "      <td>130.028572</td>\n",
       "      <td>46.473303</td>\n",
       "      <td>46.473303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2009</td>\n",
       "      <td>4</td>\n",
       "      <td>75.014431</td>\n",
       "      <td>130.028572</td>\n",
       "      <td>103.512464</td>\n",
       "      <td>103.512464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Index  Grain          X           y        y_hat    y_hat_adj\n",
       "0    2000      1  41.702200  988.861089  1003.046507  1003.046507\n",
       "1    2000      2  72.032449  988.861089  1034.655128  1034.655128\n",
       "2    2000      3   0.011437  988.861089   952.220090   952.220090\n",
       "3    2000      4  30.233257  988.861089   965.522630   965.522630\n",
       "4    2001      1  14.675589  748.165654   922.785151   922.785151\n",
       "5    2001      2   9.233859  748.165654   834.762690   834.762690\n",
       "6    2001      3  18.626021  748.165654   706.284504   706.284504\n",
       "7    2001      4  34.556073  748.165654   528.830263   528.830263\n",
       "8    2002      1  39.676747  280.443992   284.593228   284.593228\n",
       "9    2002      2  53.881673  280.443992   193.063419   193.063419\n",
       "10   2002      3  41.919451  280.443992   218.084823   218.084823\n",
       "11   2002      4  68.521950  280.443992   426.034522   426.034522\n",
       "12   2003      1  20.445225  789.279328   700.815240   700.815240\n",
       "13   2003      2  87.811744  789.279328   925.029879   925.029879\n",
       "14   2003      3   2.738759  789.279328   824.018509   824.018509\n",
       "15   2003      4  67.046751  789.279328   707.253660   707.253660\n",
       "16   2004      1  41.730480  103.226007   329.651025   305.039191\n",
       "17   2004      2  55.868983  103.226007   116.567842   107.864855\n",
       "18   2004      3  14.038694  103.226007   -29.862173     0.000000\n",
       "19   2004      4  19.810149  103.226007    -3.452649     0.000000\n",
       "20   2005      1  80.074457  447.893526   202.864336   202.864336\n",
       "21   2005      2  96.826158  447.893526   380.244889   380.244889\n",
       "22   2005      3  31.342418  447.893526   489.000452   489.000452\n",
       "23   2005      4  69.232262  447.893526   719.464430   719.464430\n",
       "24   2006      1  87.638915  908.595503   945.684065   945.684065\n",
       "25   2006      2  89.460666  908.595503  1032.165034  1032.165034\n",
       "26   2006      3   8.504421  908.595503   911.044299   911.044299\n",
       "27   2006      4   3.905478  908.595503   745.488595   745.488595\n",
       "28   2007      1  16.983042  293.614148   475.312865   475.312865\n",
       "29   2007      2  87.814250  293.614148   352.729447   352.729447\n",
       "30   2007      3   9.834683  293.614148   165.916212   165.916212\n",
       "31   2007      4  42.110763  293.614148   180.498083   180.498083\n",
       "32   2008      1  95.788953  287.775339   305.366540   305.366540\n",
       "33   2008      2  53.316528  287.775339   296.380042   296.380042\n",
       "34   2008      3  69.187711  287.775339   311.953908   311.953908\n",
       "35   2008      4  31.551563  287.775339   237.400858   237.400858\n",
       "36   2009      1  68.650093  130.028572   204.211173   204.211173\n",
       "37   2009      2  83.462567  130.028572   165.917351   165.917351\n",
       "38   2009      3   1.828828  130.028572    46.473303    46.473303\n",
       "39   2009      4  75.014431  130.028572   103.512464   103.512464"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate test data\n",
    "np.random.seed(1)  # Set a random seed for reproducibility\n",
    "df_test = pd.DataFrame({\n",
    "    \"Index\": np.repeat(np.arange(2000, 2010), 4),  # Repeats each year 4 times (quarterly data)\n",
    "    \"Grain\": np.tile(np.arange(1, 5), 10),  # Cycles through 1 to 4 for each year\n",
    "    \"X\": np.random.rand(40) * 100,  # High-frequency indicator series (random values scaled to 100)\n",
    "    \"y\": np.repeat(np.random.rand(10) * 1000, 4)  # Low-frequency series (constant per year, scaled to 1000)\n",
    "})\n",
    "\n",
    "\n",
    "# Crear instancia de la clase con conversi√≥n por promedio\n",
    "td = TemporalDisaggregation(conversion=\"average\")\n",
    "\n",
    "# Realizar la predicci√≥n en un solo paso\n",
    "td.predict(df_test, method=\"chow-lin-opt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class TemporalAggregation:\n",
    "    \"\"\"\n",
    "    Class for temporal aggregation of high-frequency time series into low-frequency series.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conversion=\"sum\"):\n",
    "        \"\"\"\n",
    "        Initializes the TemporalAggregation class.\n",
    "\n",
    "        Parameters:\n",
    "            conversion (str): Specifies the aggregation method:\n",
    "                - \"sum\": Sum of the high-frequency values.\n",
    "                - \"average\": Mean of the high-frequency values.\n",
    "                - \"first\": First observed value.\n",
    "                - \"last\": Last observed value.\n",
    "        \"\"\"\n",
    "        if conversion not in [\"sum\", \"average\", \"first\", \"last\"]:\n",
    "            raise ValueError(\"Invalid conversion method. Choose from 'sum', 'average', 'first', 'last'.\")\n",
    "        \n",
    "        self.conversion = conversion\n",
    "\n",
    "    def aggregate(self, df, time_col, value_col, freq):\n",
    "        \"\"\"\n",
    "        Aggregates high-frequency data into low-frequency data.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame containing time series data.\n",
    "            time_col (str): Column representing the time index.\n",
    "            value_col (str): Column with values to aggregate.\n",
    "            freq (str): Target frequency ('M' for monthly, 'Q' for quarterly, 'A' for annual).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Aggregated time series.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "        df.set_index(time_col, inplace=True)\n",
    "        \n",
    "        if self.conversion == \"sum\":\n",
    "            aggregated = df[value_col].resample(freq).sum()\n",
    "        elif self.conversion == \"average\":\n",
    "            aggregated = df[value_col].resample(freq).mean()\n",
    "        elif self.conversion == \"first\":\n",
    "            aggregated = df[value_col].resample(freq).first()\n",
    "        elif self.conversion == \"last\":\n",
    "            aggregated = df[value_col].resample(freq).last()\n",
    "        \n",
    "        return aggregated.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retropolation:\n",
    "    \"\"\"\n",
    "    Class for extending time series backwards using various statistical and machine learning methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, new_col, old_col):\n",
    "        \"\"\"\n",
    "        Initializes the Retropolation class.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame containing the time series.\n",
    "            new_col (str): Column with the new methodology series.\n",
    "            old_col (str): Column with the old methodology series.\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.new_col = new_col\n",
    "        self.old_col = old_col\n",
    "\n",
    "        # Fill missing values using linear interpolation\n",
    "        self.df[self.new_col] = self.df[self.new_col].interpolate(method='linear')\n",
    "        self.df[self.old_col] = self.df[self.old_col].interpolate(method='linear')\n",
    "\n",
    "    def _proportion(self, mask):\n",
    "        \"\"\"Applies proportion-based retropolation.\"\"\"\n",
    "        proportion = self.df[self.new_col].dropna() / self.df[self.old_col].dropna()\n",
    "        mean_prop = np.mean(proportion)\n",
    "        self.df.loc[mask, self.new_col] = mean_prop * self.df.loc[mask, self.old_col]\n",
    "\n",
    "    def _linear_regression(self, mask):\n",
    "        \"\"\"Applies linear regression for retropolation.\"\"\"\n",
    "        valid_data = self.df.dropna(subset=[self.new_col, self.old_col])\n",
    "        if valid_data.empty:\n",
    "            raise ValueError(\"Not enough data for linear regression.\")\n",
    "        \n",
    "        model = LinearRegression().fit(valid_data[[self.old_col]], valid_data[self.new_col])\n",
    "        self.df.loc[mask, self.new_col] = model.predict(self.df.loc[mask, self.old_col].values.reshape(-1, 1))\n",
    "\n",
    "    def _polynomial_regression(self, mask):\n",
    "        \"\"\"Applies polynomial regression for retropolation.\"\"\"\n",
    "        valid_data = self.df.dropna(subset=[self.new_col, self.old_col])\n",
    "        if valid_data.empty:\n",
    "            raise ValueError(\"Not enough data for polynomial regression.\")\n",
    "        \n",
    "        poly_model = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "        params = {'polynomialfeatures__degree': range(1, 6)}\n",
    "        grid = GridSearchCV(poly_model, params, cv=5)\n",
    "        grid.fit(valid_data[[self.old_col]], valid_data[self.new_col])\n",
    "        self.df.loc[mask, self.new_col] = grid.predict(self.df.loc[mask, self.old_col].values.reshape(-1, 1))\n",
    "\n",
    "    def _exponential_smoothing(self, mask, alpha=0.5):\n",
    "        \"\"\"Applies exponential smoothing for retropolation.\"\"\"\n",
    "        smoothed = self.df[self.new_col].ewm(alpha=alpha, adjust=False).mean()\n",
    "        self.df.loc[mask, self.new_col] = smoothed.iloc[-1]\n",
    "\n",
    "    def _mlp_regression(self, mask):\n",
    "        \"\"\"Applies MLP regression for retropolation.\"\"\"\n",
    "        valid_data = self.df.dropna(subset=[self.new_col, self.old_col])\n",
    "        if valid_data.empty:\n",
    "            raise ValueError(\"Not enough data for MLP regression.\")\n",
    "\n",
    "        model = MLPRegressor(hidden_layer_sizes=(100,), max_iter=5000, activation=\"relu\", alpha=0.001, random_state=42)\n",
    "        model.fit(valid_data[[self.old_col]], valid_data[self.new_col])\n",
    "        self.df.loc[mask, self.new_col] = model.predict(self.df.loc[mask, self.old_col].values.reshape(-1, 1))\n",
    "\n",
    "    def retropolate(self, method=\"proportion\"):\n",
    "        \"\"\"\n",
    "        Performs retropolation using the specified method.\n",
    "\n",
    "        Parameters:\n",
    "            method (str): Retropolation method. Options:\n",
    "                - \"proportion\"\n",
    "                - \"linear_regression\"\n",
    "                - \"polynomial_regression\"\n",
    "                - \"exponential_smoothing\"\n",
    "                - \"mlp_regression\"\n",
    "\n",
    "        Returns:\n",
    "            pd.Series: Completed series with retropolated values.\n",
    "        \"\"\"\n",
    "        mask = self.df[self.new_col].isna() & self.df[self.old_col].notna()\n",
    "\n",
    "        if method == \"proportion\":\n",
    "            self._proportion(mask)\n",
    "        elif method == \"linear_regression\":\n",
    "            self._linear_regression(mask)\n",
    "        elif method == \"polynomial_regression\":\n",
    "            self._polynomial_regression(mask)\n",
    "        elif method == \"exponential_smoothing\":\n",
    "            self._exponential_smoothing(mask)\n",
    "        elif method == \"mlp_regression\":\n",
    "            self._mlp_regression(mask)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid method. Choose from the available options.\")\n",
    "\n",
    "        return self.df[self.new_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
